Useful Tidbits

# Wrong $p$-norm Error Bound

  Let $X^{(n\ \times\ d)} \subset \mathbb{R}^d.$ Let
    $$\begin{align}
        v_1 &= \max_{\substack{\|v\| = 1 \\ v \in \mathbb{R}^d}} \|X v\|_1, \text{ and } \\
        v_2 &= \max_{\substack{\|v\| = 1 \\ v \in \mathbb{R}^d}} \|X v\|_2.
      \end{align}$$

  Notice that $v_2$ is the first principle component of $X.$ Now we have
    $$ \frac{\|Xv_1\|_1}{\sqrt{d}} \leq \|Xv_1\|_2 \leq \|Xv_2\|_2 \leq \|X v_2\|_1 \leq \|Xv_1\|_1, $$

  which implies
    $$ \|Xv_1\|_1 \leq \sqrt{d} \|Xv_2\|_1, $$

  and also
    $$\begin{align}
        \|Xv_1\|_1 - \|Xv_2\|_1 &\leq \|Xv_1\|_1\big(1 - \textstyle{\frac{1}{\sqrt{d}}}\big) \\
        &\leq \sqrt{d} \|Xv_2\|_1\big(1 - \textstyle{\frac{1}{\sqrt{d}}}\big) \\
        &= \|Xv_2\|_1 (\sqrt{d} - 1).
      \end{align}$$

  This final inequality is a bound on the maximum difference between the absolute sum ($1$-norm) of the $2$-norm maximizing vector and the correct $1$-norm maximizing vector.

$ $

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Equivalence of PCA Under Difference Operator

  Given $X$ as stated above, if we consider the set of all pairwise difference vectors $Z,$ then $PCA(Z) = PCA(X).$ Denote $Z_j$ to be the subset of $Z$ about vertex $x_j \in X,$ $Z_j = \{x_j - x_i : x_i \in X\}.$

  We will analyze this by considering the first principle component of $X,$ $v_1 \in \mathbb{R}^d.$ We will note that the first principle component of $Z$ is the same.

    $$\begin{align}
      &\max_{\|v\|_2 = 1} \|Zv\|_2 \\
      = &\max_{\|v\|_2 = 1} \|Zv\|_2^2 \\
      = &\max_{\|v\|_2 = 1} \textstyle{\sum_{z \in Z}} \langle z, v \rangle^2 \\
      = &\max_{\|v\|_2 = 1} \textstyle{\sum_{Z_j \subset Z} \sum_{z \in Z_j}} \langle z, v \rangle^2 \\
      \leq &\ \textstyle{\sum_{Z_j \subset Z} \max_{\|v\|_2 = 1} \sum_{z \in Z_j}} \langle z, v \rangle^2 \\
      = &\ \textstyle{\sum_{Z_j \subset Z} \sum_{z \in Z_j}} \langle z, v_1 \rangle^2 \\
      = &\ \textstyle{\sum_{z \in Z}} \langle z, v_1 \rangle^2 \\
      = &\ \|Zv_1\|_2 \\
      \\
      \implies \|Zv_1\|_2 &= \displaystyle{\max_{\|v\|_2 = 1}} \|Zv\|_2.
     \end{align}$$

  The same methodology extended to the remaining components achieves total equivalence of the principle components for $X$ and $Z.$

$ $

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
# Direction of Maximum Average Change

## Theorem

  Let $S \subset \mathbb{R}^d$ be open and convex, let $f: \mathbb{R}^d \rightarrow \mathbb{R}$ with $\nabla f$ being $\gamma$-Lipschitz continuous under the $2$-norm in $S.$ Let $X = [x_1,$ $x_2,$ $\ldots,$ $x_n],$ $x_i \in S.$ We denote $X_k = [x_1,$ $\ldots,$ $x_k]$ for $k < n.$ Now, let $M_{X_k}$ be the matrix that conditions the vectors in $X_k$ such that the conditioning of the set of between vectors is maximized.



$ $

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Maximizing the One-Norm

$\DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}$ The goal, stated formally is to compute 

  $$ \max_{\|v\|_2 = 1} \big \| X^T v \big \|_1, $$

given $X = [x_1,$ $\ldots,$ $x_n],$ $v, x_i \in \mathbb{R}^d.$ We start with the following theorem.

## Theorem 2
  Let $X = \{x^{(1)},$ $\ldots,$ $x^{(n)}\},$ $x^{(i)} \in \mathbb{R}^d,$ then

  $$ \argmax_{\|v\|_2 = 1} \sum_{x^{(i)} \in X} \langle x^{(i)}, v \rangle = \frac{\sum_{x^{(i)} \in X} x^{(i)}}{\big \| \sum_{x^{(i)} \in X} x^{(i)} \big \|_2}. $$

* **Proof** *
  Let $c \in \mathbb{R}^d,$ such that $c_j = \sum_{x^{(i)} \in X} x^{(i)}_j,$ $1 \leq j \leq d.$

  $$\begin{align}
      \argmax_{\|v\|_2 = 1} \sum_{x^{(i)} \in X} \langle x^{(i)}, v \rangle &= \argmax_{\|v\|_2 = 1} \sum_{1 \leq j \leq d} \sum_{x^{(i)} \in X} v_j x_j^{(i)} \\
      &= \argmax_{\|v\|_2 = 1} \sum_{1 \leq j \leq d} v_j \sum_{x^{(i)} \in X} x_j^{(i)} \\
      &= \argmax_{\|v\|_2 = 1} \sum_{1 \leq j \leq d} v_j c_j \\
      &= \argmax_{\|v\|_2 = 1} \langle v, c \rangle \\
      &= \big \langle \textstyle{\frac{c}{\|c\|_2}}, c \big \rangle \\
      \\
      \implies v &= \frac{\sum_{x^{(i)} \in X} x^{(i)}}{\big \| \sum_{x^{(i)} \in X} x^{(i)} \big \|_2}.
    \end{align} $$

  $\square$

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let $v_1$ be the unit length vector that achieves the maximum value of $\| X^t v \|_1,$ $v \in \mathbb{R}^d.$ Now we have

  $$\begin{align}
      v_1 &= \argmax_{\|v\|_2 = 1} \| X^t v \|_1 \\
          &= \argmax_{\|v\|_2 = 1} \sum_{x_i \in X} |\langle x_i, v \rangle| \\
          &= \argmax_{\|v\|_2 = 1} \sum_{x_i \in X} \textstyle{\frac{|\langle x_i, v \rangle|}{\langle x_i, v \rangle}} \langle x_i, v \rangle \\
        \\
        \implies v_1 &= \frac{\sum_{x_i \in X} \textstyle{\frac{|\langle x_i, v_1 \rangle|}{\langle x_i, v_1 \rangle}} x_i}{\big \| \sum_{x_i \in X} \textstyle{\frac{|\langle x_i, v_1 \rangle|}{\langle x_i, v_1 \rangle}} x_i  \big \|_2}, \\
    \end{align}$$

where the last step is achieved by applying @@Theorem 1@@ to the set of sign-adjusted vectors in $X.$

From this necessary condition for achieving the maximum, one may construct the iterative technique

  $$ v_{t+1} = \sum_{x_i} \big( \textstyle{\frac{|\langle x_i, v_t \rangle|}{\langle x_i, v_t \rangle}} \big) x_i \ \bigg / \  \bigg \| \displaystyle{\sum_{x_i}} \big( \textstyle{\frac{|\langle x_i, v_t \rangle|}{\langle x_i, v_t \rangle}} \big) x_i \bigg \|_2. $$

Unfortunately, it is possible for this iterative technique to fail to converge on the maximum.

%% However, if we simultaneously try to compute $d$ orthonormal vectors convergence is once again guaranteed. Consider the following theorem.

%% # Theorem 2
%%   Iterations of the form $\ldots$ converge on the $d$ one-components.

%% * **Proof** *
%%   Something impressive.



%% A somewhat obvious upper bound can be calculated,

%%   $$\begin{align}
%%      \max_{\|v\|_2 = 1} \big \| X^T v \big \|_1 &= \max_{\|v\|_2 = 1} \sum_{x_i \in X} |\langle x_i, v \rangle| \\
%%      &\leq \sum_{x_i \in X} \max_{\|v\|_2 = 1} |\langle x_i, v \rangle| \\
%%      &= \sum_{x_i \in X} \|x_i\|_2. \\
%%     \end{align}$$

